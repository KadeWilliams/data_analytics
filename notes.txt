
           2/3
         -------> Training Data _______> Model |
Data ----|
         ------->  Testing Data _______> Performance of Model ----> Confusion Matrix [TP|FP
                                                                                      FN|TN]
           1/3 

Model - Simplified representation of reality that is trained using the training data that is then applied to the testing data 
      E.g. Decision Model

ROC: Receiver Operating Characteristics
    - What TP BIG and FP small
    - AUC -> Area Under the ROC Curve
        - Produces values from 0 to 1.0
        - Random chance is .5 and perfect classification is 1.0
        - Produces a good assessment for skewed class distributions too!
        - Criterion for AUROC
            -> |.9-1.0| = Very Good |
            -> |.8-.9 | = Good |
            -> |.7-.8 | = Fair |
            -> |.6-.7 | = Poor |
            -> |.5-.6 | = Fail |
Decision Trees
    - Top is called the "root" node and is then split into three nodes 
    - Any additional data will bring additional information making the model more accurate 
    - Nodes that do not have children are called leaves because they don't branch off and end 

Supervised Segmentation
    - How can we segment the population into groups that differ from each with respect to some quantity of interest?
    - How can we judge which attributes are more important to serve as the root node to split the tree? Can we compare attributes in terms of their importance for building the tree?

Criterion for Attribute Selection:
    - Which is the best attribute?
        - Want to get the smallest tree
        - Heuristic: choose the attribute that produces the "purest" nodes
    - Popular impurity criterion: entropy
        - Information gain increases with the average purity of the subsets
        - The measure of disorder, uncertainty
        - By getting more information we can reduce entropy
    - Strategy: choose the attribute that gives the greatest information gain
        - I.e. what reduces the entropy the most

Entropy (H: Greek letter Eta): Often interpreted as the degree of disorder or randomness in the system
    - Measure information in bits:
        - Given a probability distribution, the info required to predict an event is the distribution's entropy
        - Entropy gives the information required in bits (can involve fractions of bits!)
    - Formula for computing entropy:
        - entropy(p1, p2, ... p3) = -p1 log p1 - ... - pn log pn

Selecting Informative attributes
    - The most common splitting criterion is called information gain (IG)
        - It is based on a purity measure called entropy
            - Measures the general disorder of a set 
            - All positives or all negatives have the lowest level of entropy, a mixed set means you have a large amount of entropy

A general algorithm for building a Decision Tree
    - Employs a divide and conquer method
    - Recursively divides a training set until each division consists of examples from one class:
        - Create a root node and assign all of the training data to it
        - Select the best splitting attribute
        - Add a branch to the root node for each value of the split. Split the data into mutually exclusive subsets along the lines of the specific split.
        - Repeat steps 2 and 3 for each and every leaf node until the stopping criteria is reached

Information Gain
    - Information gain measures the change in entropy due to any amount of new information being added 
    - Entropy = -p1 log2 p1 - p2 log2 p2
        - Parent Node Entropy ->  14/30 log2 14/30 - 16/30 log2 16/30 ===> .99 (very impure, very uncertain)
        - You then get the children's entropy values and finally find the weighted entropy value of all the nodes 
            - Weight -> the number of occurances relative to the sample 
                - Smaller Nodes have a Smaller Weight
                - Larger Nodes have a Larger Weight
        - Weighted
            - p1 * percent of total + p2 * percent of total ===> Information Gained 
Some Simple Observations
    - When the number of either yes's or no's is zero entropy is zero
    - When the number of yes's or no's is equal, entropy reaches a maximum

Entropy should be reduced, and used, as much as possible when picking predictive models

Another way: Gini Index
    - Used in economics to measure the diversity of a population and wealth gap
    - Can be used to determine the purity of a specific class as a result of a decision to branch along a particular attribute or variable
    - Gini = 
    - Where p is the relative frequency of class j in s

Major algorithms of decision Trees:
    - C4.5/C5.0 (J48)
    - Random Forest
    - Native Baynes classifier
    - CHAID: Chi-squared Automatic Interaction Detection

Perfect models should be avoided because the performance of the model improves with errors
    - you can over-fit your model to the dataset

Continuous data = Linear Regression 
Categorical data = Logistic Regression, J48, RandomForest 

The best practice is to try them all and to choose the best one to make predictions 